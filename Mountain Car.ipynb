{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled33.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "FBc_bpPT9R1M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential  \n",
        "from keras.layers import Dense, Flatten\n",
        "from collections import deque \n",
        "import numpy as np\n",
        "import gym \n",
        "env = gym.make('MountainCar-v0')  \n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I3iScSkm9Snu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(20, input_shape=(2,) + env.observation_space.shape, init='uniform', activation='relu'))\n",
        "model.add(Flatten())       # Flatten input so as to have no problems with processing\n",
        "model.add(Dense(18, init='uniform', activation='relu'))\n",
        "model.add(Dense(10, init='uniform', activation='relu'))\n",
        "model.add(Dense(env.action_space.n, init='uniform', activation='linear'))\n",
        "model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W-iTXIPl9bTG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "D = deque()\n",
        "observetime = 500\n",
        "epsilon = 0.7\n",
        "gamma = 0.9\n",
        "mb_size = 50  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WLAjAFNS9eQc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "observation = env.reset() \n",
        "obs = np.expand_dims(observation, axis=0) \n",
        "state = np.stack((obs, obs), axis=1)\n",
        "done = False\n",
        "for t in range(observetime):\n",
        "    if np.random.rand() <= epsilon:\n",
        "        action = np.random.randint(0, env.action_space.n, size=1)[0]\n",
        "    else:\n",
        "        Q = model.predict(state)\n",
        "        action = np.argmax(Q)\n",
        "    observation_new, reward, done, info = env.step(action)\n",
        "    obs_new = np.expand_dims(observation_new, axis=0)          \n",
        "    state_new = np.append(np.expand_dims(obs_new, axis=0), state[:, :1, :], axis=1)     \n",
        "    D.append((state, action, reward, state_new, done))         \n",
        "    state = state_new         \n",
        "    if done:\n",
        "        env.reset()           \n",
        "        obs = np.expand_dims(observation, axis=0)      \n",
        "        state = np.stack((obs, obs), axis=1)\n",
        "print('Observing Finished')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dsh0W-hH9i5F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "minibatch = random.sample(D, mb_size)                              \n",
        "inputs_shape = (mb_size,) + state.shape[1:]\n",
        "inputs = np.zeros(inputs_shape)\n",
        "targets = np.zeros((mb_size, env.action_space.n))\n",
        "\n",
        "for i in range(0, mb_size):\n",
        "    state = minibatch[i][0]\n",
        "    action = minibatch[i][1]\n",
        "    reward = minibatch[i][2]\n",
        "    state_new = minibatch[i][3]\n",
        "    done = minibatch[i][4]\n",
        "    inputs[i:i+1] = np.expand_dims(state, axis=0)\n",
        "    targets[i] = model.predict(state)\n",
        "    Q_sa = model.predict(state_new)\n",
        "    \n",
        "    if done:\n",
        "        targets[i, action] = reward\n",
        "    else:\n",
        "        targets[i, action] = reward + gamma * np.max(Q_sa)\n",
        "\n",
        "    model.train_on_batch(inputs, targets)\n",
        "print('Learning Finished')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UK4yu7-79sGA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "observation = env.reset()\n",
        "obs = np.expand_dims(observation, axis=0)\n",
        "state = np.stack((obs, obs), axis=1)\n",
        "done = False\n",
        "tot_reward = 0.0\n",
        "while not done:\n",
        "    env.render()\n",
        "    Q = model.predict(state)        \n",
        "    action = np.argmax(Q)         \n",
        "    observation, reward, done, info = env.step(action)\n",
        "    obs = np.expand_dims(observation, axis=0)\n",
        "    state = np.append(np.expand_dims(obs, axis=0), state[:, :1, :], axis=1)    \n",
        "    tot_reward += reward\n",
        "print('Game ended! Total reward: {}'.format(reward))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}